{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the path where telecom_analysis.py is located\n",
    "sys.path.append(os.path.abspath('../scripts'))\n",
    "\n",
    "\n",
    "# Import functions from telecom_analysis.py\n",
    "from data_preprocessing import (\n",
    "    load_data,\n",
    "    data_overview,\n",
    "    standardize_numerical_features,\n",
    "    encode_categorical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_file_path = '../data/creditcard.csv'\n",
    "fraud_data_file_path = '../data/Fraud_Data.csv'\n",
    "ipaddress_to_country_file_path = '../data/IpAddress_to_Country.csv'\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "creditcard_df = pd.read_csv(creditcard_file_path)\n",
    "fraud_data_df = pd.read_csv(fraud_data_file_path)\n",
    "ipaddress_to_country_df = pd.read_csv(ipaddress_to_country_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_df.head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipaddress_to_country_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Credit Card data:\n",
      " Time      0\n",
      "V1        0\n",
      "V2        0\n",
      "V3        0\n",
      "V4        0\n",
      "V5        0\n",
      "V6        0\n",
      "V7        0\n",
      "V8        0\n",
      "V9        0\n",
      "V10       0\n",
      "V11       0\n",
      "V12       0\n",
      "V13       0\n",
      "V14       0\n",
      "V15       0\n",
      "V16       0\n",
      "V17       0\n",
      "V18       0\n",
      "V19       0\n",
      "V20       0\n",
      "V21       0\n",
      "V22       0\n",
      "V23       0\n",
      "V24       0\n",
      "V25       0\n",
      "V26       0\n",
      "V27       0\n",
      "V28       0\n",
      "Amount    0\n",
      "Class     0\n",
      "dtype: int64\n",
      "Missing values in Fraud Data:\n",
      " user_id           0\n",
      "signup_time       0\n",
      "purchase_time     0\n",
      "purchase_value    0\n",
      "device_id         0\n",
      "source            0\n",
      "browser           0\n",
      "sex               0\n",
      "age               0\n",
      "ip_address        0\n",
      "class             0\n",
      "dtype: int64\n",
      "Missing values in IP Address to Country Data:\n",
      " lower_bound_ip_address    0\n",
      "upper_bound_ip_address    0\n",
      "country                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each dataset\n",
    "print(\"Missing values in Credit Card data:\\n\", creditcard_df.isnull().sum())\n",
    "print(\"Missing values in Fraud Data:\\n\", fraud_data_df.isnull().sum())\n",
    "print(\"Missing values in IP Address to Country Data:\\n\", ipaddress_to_country_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we will drop rows with missing values (you can use imputation if needed)\n",
    "creditcard_df.dropna(inplace=True)\n",
    "fraud_data_df.dropna(inplace=True)\n",
    "ipaddress_to_country_df.dropna(inplace=True)\n",
    "\n",
    "# 2. Data Cleaning\n",
    "# Remove duplicates in each dataset\n",
    "creditcard_df.drop_duplicates(inplace=True)\n",
    "fraud_data_df.drop_duplicates(inplace=True)\n",
    "ipaddress_to_country_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types where appropriate\n",
    "# For Fraud Data: Convert 'signup_time' and 'purchase_time' to datetime format\n",
    "fraud_data_df['signup_time'] = pd.to_datetime(fraud_data_df['signup_time'])\n",
    "fraud_data_df['purchase_time'] = pd.to_datetime(fraud_data_df['purchase_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in Fraud Data:\n",
      " user_id                    int64\n",
      "signup_time       datetime64[ns]\n",
      "purchase_time     datetime64[ns]\n",
      "purchase_value             int64\n",
      "device_id                 object\n",
      "source                    object\n",
      "browser                   object\n",
      "sex                       object\n",
      "age                        int64\n",
      "ip_address               float64\n",
      "class                      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types in Fraud Data:\\n\", fraud_data_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "# --- Credit Card Dataset EDA ---\n",
    "# Univariate Analysis - Distribution of transaction amounts\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(creditcard_df['Amount'], bins=50, kde=True)\n",
    "plt.title('Distribution of Transaction Amounts (Credit Card Data)')\n",
    "plt.show()\n",
    "\n",
    "# Univariate Analysis - Count of fraudulent vs non-fraudulent transactions\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Class', data=creditcard_df)\n",
    "plt.title('Fraudulent vs Non-Fraudulent Transactions (Credit Card Data)')\n",
    "plt.show()\n",
    "\n",
    "# --- Fraud Data EDA ---\n",
    "# Univariate Analysis - Distribution of purchase values\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(fraud_data_df['purchase_value'], bins=50, kde=True)\n",
    "plt.title('Distribution of Purchase Values (Fraud Data)')\n",
    "plt.show()\n",
    "\n",
    "# Univariate Analysis - Distribution of age\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(fraud_data_df['age'], bins=30, kde=True)\n",
    "plt.title('Distribution of Age (Fraud Data)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis - Correlation between numeric features in Credit Card Data\n",
    "plt.figure(figsize=(12,8))\n",
    "corr_matrix = creditcard_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap (Credit Card Data)')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate Analysis - Fraud occurrence by gender in Fraud Data\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='sex', hue='class', data=fraud_data_df)\n",
    "plt.title('Fraud Occurrence by Gender (Fraud Data)')\n",
    "plt.show()\n",
    "\n",
    "# Time difference between signup and purchase\n",
    "fraud_data_df['time_diff'] = (fraud_data_df['purchase_time'] - fraud_data_df['signup_time']).dt.total_seconds() / 3600\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(fraud_data_df['time_diff'], bins=50, kde=True)\n",
    "plt.title('Distribution of Time Difference between Signup and Purchase (Fraud Data)')\n",
    "plt.show()\n",
    "\n",
    "# --- IP Address to Country Dataset EDA ---\n",
    "# Check distribution of countries\n",
    "plt.figure(figsize=(12,6))\n",
    "top_countries = ipaddress_to_country_df['country'].value_counts().head(10)\n",
    "sns.barplot(x=top_countries.index, y=top_countries.values)\n",
    "plt.title('Top 10 Countries in IP Address Range')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Data (with dot-decimal IPs):\n",
      "   user_id     ip_address\n",
      "0    22058    43.173.1.96\n",
      "1   333320  20.225.83.219\n",
      "2     1359  156.64.132.28\n",
      "3   150084  228.234.6.235\n",
      "4   221365  24.197.75.141\n",
      "\n",
      "IP Address to Country Data:\n",
      "  lower_bound_ip_address upper_bound_ip_address    country\n",
      "0                1.0.0.0              1.0.0.255  Australia\n",
      "1                1.0.1.0              1.0.1.255      China\n",
      "2                1.0.2.0              1.0.3.255      China\n",
      "3                1.0.4.0              1.0.7.255  Australia\n",
      "4                1.0.8.0             1.0.15.255      China\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to convert an integer to a dot-decimal IP format\n",
    "def int_to_ip(ip_int):\n",
    "    \"\"\"\n",
    "    Converts an integer IP address back to the dot-decimal format (e.g., '192.168.0.1').\n",
    "    \"\"\"\n",
    "    # Ensure ip_int is an integer\n",
    "    if not isinstance(ip_int, int):\n",
    "        raise ValueError(f\"Expected an integer, got {type(ip_int).__name__}\")\n",
    "\n",
    "    return '.'.join([str((ip_int >> (i * 8)) & 0xFF) for i in range(3, -1, -1)])\n",
    "\n",
    "\n",
    "# Step 1: Ensure the IP address columns in both DataFrames are treated as integers\n",
    "fraud_data_df['ip_address'] = fraud_data_df['ip_address'].fillna(0).astype(int)\n",
    "ipaddress_to_country_df['lower_bound_ip_address'] = ipaddress_to_country_df['lower_bound_ip_address'].fillna(0).astype(int)\n",
    "ipaddress_to_country_df['upper_bound_ip_address'] = ipaddress_to_country_df['upper_bound_ip_address'].fillna(0).astype(int)\n",
    "\n",
    "# Step 2: Convert the integer IP addresses to dot-decimal format (overwrite the columns)\n",
    "ipaddress_to_country_df['lower_bound_ip_address'] = ipaddress_to_country_df['lower_bound_ip_address'].apply(int_to_ip)\n",
    "ipaddress_to_country_df['upper_bound_ip_address'] = ipaddress_to_country_df['upper_bound_ip_address'].apply(int_to_ip)\n",
    "fraud_data_df['ip_address'] = fraud_data_df['ip_address'].apply(int_to_ip)\n",
    "\n",
    "# Step 3: Inspect the results\n",
    "print(\"Fraud Data (with dot-decimal IPs):\")\n",
    "print(fraud_data_df[['user_id', 'ip_address']].head())  # Show the fraud data IPs\n",
    "print(\"\\nIP Address to Country Data:\")\n",
    "print(ipaddress_to_country_df.head())  # Show the converted IP address ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id         signup_time       purchase_time  purchase_value  \\\n",
      "634   247547 2015-06-28 03:00:34 2015-08-09 03:57:29              47   \n",
      "635   220737 2015-01-28 14:21:11 2015-02-11 20:28:28              15   \n",
      "636   390400 2015-03-19 20:49:09 2015-04-11 23:41:23              44   \n",
      "637    69592 2015-02-24 06:11:57 2015-05-23 16:40:14              55   \n",
      "638   174987 2015-07-07 12:58:11 2015-11-03 04:04:30              51   \n",
      "\n",
      "         device_id  source browser sex  age  ip_address  class    country  \n",
      "634  KIXYSVCHIPQBR     SEO  Safari   F   30    16778864      0  Australia  \n",
      "635  PKYOWQKWGJNJI     SEO  Chrome   F   34    16842045      0   Thailand  \n",
      "636  LVCSXLISZHVUO     Ads      IE   M   29    16843656      0      China  \n",
      "637  UHAUHNXXUADJE  Direct  Chrome   F   30    16938732      0      China  \n",
      "638  XPGPMOHIDRMGE     SEO  Chrome   F   37    16971984      0   Thailand  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to convert IP address from dot-decimal format to integer format\n",
    "def ip_to_int(ip_str):\n",
    "    \"\"\"\n",
    "    Converts a dot-decimal IP address (e.g., '192.168.0.1') to its integer equivalent.\n",
    "    \"\"\"\n",
    "    octets = ip_str.split('.')\n",
    "    return (int(octets[0]) << 24) + (int(octets[1]) << 16) + (int(octets[2]) << 8) + int(octets[3])\n",
    "\n",
    "\n",
    "# Step 2: Convert IP addresses to integer format\n",
    "fraud_data_df['ip_address'] = fraud_data_df['ip_address'].apply(ip_to_int)\n",
    "ipaddress_to_country_df['lower_bound_ip_address'] = ipaddress_to_country_df['lower_bound_ip_address'].apply(ip_to_int)\n",
    "ipaddress_to_country_df['upper_bound_ip_address'] = ipaddress_to_country_df['upper_bound_ip_address'].apply(ip_to_int)\n",
    "\n",
    "# Step 3: Merge the two datasets based on IP address ranges\n",
    "# We want to find the rows where fraud_data_df['ip_address'] is between lower_bound_ip_address and upper_bound_ip_address\n",
    "fraud_data_df= pd.merge_asof(\n",
    "    fraud_data_df.sort_values('ip_address'),\n",
    "    ipaddress_to_country_df.sort_values('lower_bound_ip_address'),\n",
    "    left_on='ip_address',\n",
    "    right_on='lower_bound_ip_address',\n",
    "    direction='backward'  # Ensures we find the closest lower_bound_ip_address <= ip_address\n",
    ")\n",
    "\n",
    "# Step 4: Filter rows where ip_address falls between lower_bound and upper_bound\n",
    "fraud_data_df = fraud_data_df[(fraud_data_df['ip_address'] >= fraud_data_df['lower_bound_ip_address']) & \n",
    "                      (fraud_data_df['ip_address'] <= fraud_data_df['upper_bound_ip_address'])]\n",
    "\n",
    "# Step 5: Clean up (optional, based on your needs)\n",
    "# You may drop unnecessary columns or rename them as required\n",
    "fraud_data_df = fraud_data_df.drop(columns=['lower_bound_ip_address', 'upper_bound_ip_address'])\n",
    "\n",
    "# Now, fraud_data_df contains fraud data along with the corresponding country information for each IP address\n",
    "print(fraud_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>signup_time</th>\n",
       "      <th>purchase_time</th>\n",
       "      <th>purchase_value</th>\n",
       "      <th>device_id</th>\n",
       "      <th>source</th>\n",
       "      <th>browser</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>ip_address</th>\n",
       "      <th>class</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>247547</td>\n",
       "      <td>2015-06-28 03:00:34</td>\n",
       "      <td>2015-08-09 03:57:29</td>\n",
       "      <td>47</td>\n",
       "      <td>KIXYSVCHIPQBR</td>\n",
       "      <td>SEO</td>\n",
       "      <td>Safari</td>\n",
       "      <td>F</td>\n",
       "      <td>30</td>\n",
       "      <td>16778864</td>\n",
       "      <td>0</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>220737</td>\n",
       "      <td>2015-01-28 14:21:11</td>\n",
       "      <td>2015-02-11 20:28:28</td>\n",
       "      <td>15</td>\n",
       "      <td>PKYOWQKWGJNJI</td>\n",
       "      <td>SEO</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>F</td>\n",
       "      <td>34</td>\n",
       "      <td>16842045</td>\n",
       "      <td>0</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>390400</td>\n",
       "      <td>2015-03-19 20:49:09</td>\n",
       "      <td>2015-04-11 23:41:23</td>\n",
       "      <td>44</td>\n",
       "      <td>LVCSXLISZHVUO</td>\n",
       "      <td>Ads</td>\n",
       "      <td>IE</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>16843656</td>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>69592</td>\n",
       "      <td>2015-02-24 06:11:57</td>\n",
       "      <td>2015-05-23 16:40:14</td>\n",
       "      <td>55</td>\n",
       "      <td>UHAUHNXXUADJE</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>F</td>\n",
       "      <td>30</td>\n",
       "      <td>16938732</td>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>174987</td>\n",
       "      <td>2015-07-07 12:58:11</td>\n",
       "      <td>2015-11-03 04:04:30</td>\n",
       "      <td>51</td>\n",
       "      <td>XPGPMOHIDRMGE</td>\n",
       "      <td>SEO</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>16971984</td>\n",
       "      <td>0</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id         signup_time       purchase_time  purchase_value  \\\n",
       "634   247547 2015-06-28 03:00:34 2015-08-09 03:57:29              47   \n",
       "635   220737 2015-01-28 14:21:11 2015-02-11 20:28:28              15   \n",
       "636   390400 2015-03-19 20:49:09 2015-04-11 23:41:23              44   \n",
       "637    69592 2015-02-24 06:11:57 2015-05-23 16:40:14              55   \n",
       "638   174987 2015-07-07 12:58:11 2015-11-03 04:04:30              51   \n",
       "\n",
       "         device_id  source browser sex  age  ip_address  class    country  \n",
       "634  KIXYSVCHIPQBR     SEO  Safari   F   30    16778864      0  Australia  \n",
       "635  PKYOWQKWGJNJI     SEO  Chrome   F   34    16842045      0   Thailand  \n",
       "636  LVCSXLISZHVUO     Ads      IE   M   29    16843656      0      China  \n",
       "637  UHAUHNXXUADJE  Direct  Chrome   F   30    16938732      0      China  \n",
       "638  XPGPMOHIDRMGE     SEO  Chrome   F   37    16971984      0   Thailand  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Convert the `purchase_time` and `signup_time` to datetime objects for easier manipulation\n",
    "# fraud_data_df['purchase_time'] = pd.to_datetime(fraud_data_df['purchase_time'])\n",
    "# fraud_data_df['signup_time'] = pd.to_datetime(fraud_data_df['signup_time'])\n",
    "\n",
    "# # Step 1: Transaction Frequency per User\n",
    "# # Group by `user_id` to count the number of transactions each user made\n",
    "# fraud_data_df['transaction_count'] = fraud_data_df.groupby('user_id')['user_id'].transform('count')\n",
    "\n",
    "# # Step 2: Transaction Velocity\n",
    "# # Sort data by user_id and purchase_time to calculate velocity\n",
    "# fraud_data_df = fraud_data_df.sort_values(by=['user_id', 'purchase_time'])\n",
    "\n",
    "# # Calculate time difference between consecutive purchases for the same user\n",
    "# fraud_data_df['purchase_diff'] = fraud_data_df.groupby('user_id')['purchase_time'].diff().dt.total_seconds()\n",
    "\n",
    "# # Step 3: Extracting Hour of the Day and Day of the Week from `purchase_time`\n",
    "# fraud_data_df['hour_of_day'] = fraud_data_df['purchase_time'].dt.hour\n",
    "# fraud_data_df['day_of_week'] = fraud_data_df['purchase_time'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate frequency\n",
    "fraud_data_df['transaction_frequency'] = fraud_data_df.groupby('user_id')['purchase_time'].transform('count')\n",
    "fraud_data_df['transaction_velocity'] = (fraud_data_df['purchase_time'] - fraud_data_df['signup_time']).dt.total_seconds()\n",
    "\n",
    "# Extracting hour and day from purchase time\n",
    "fraud_data_df['hour_of_day'] = fraud_data_df['purchase_time'].dt.hour\n",
    "fraud_data_df['day_of_week'] = fraud_data_df['purchase_time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4: Save the enhanced DataFrame with new features\n",
    "# fraud_data_df.to_csv('data/Fraud_Data_with_features.csv', index=False)\n",
    "\n",
    "# Display the new DataFrame head\n",
    "fraud_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns you don't want to scale, including 'class'\n",
    "exclude_columns = ['device_id', 'source', 'browser', 'sex', 'ip_address', 'class', 'country']\n",
    "\n",
    "# Apply the function\n",
    "df = standardize_numerical_features(fraud_data_df, exclude_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data_df = standardize_numerical_features(fraud_data_df)\n",
    "\n",
    "creditcard_df = standardize_numerical_features(creditcard_df)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>signup_time</th>\n",
       "      <th>purchase_time</th>\n",
       "      <th>purchase_value</th>\n",
       "      <th>device_id</th>\n",
       "      <th>source</th>\n",
       "      <th>browser</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>ip_address</th>\n",
       "      <th>class</th>\n",
       "      <th>country</th>\n",
       "      <th>transaction_frequency</th>\n",
       "      <th>transaction_velocity</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>0.411032</td>\n",
       "      <td>2015-06-28 03:00:34</td>\n",
       "      <td>2015-08-09 03:57:29</td>\n",
       "      <td>0.549607</td>\n",
       "      <td>KIXYSVCHIPQBR</td>\n",
       "      <td>SEO</td>\n",
       "      <td>Safari</td>\n",
       "      <td>F</td>\n",
       "      <td>-0.363124</td>\n",
       "      <td>16778864</td>\n",
       "      <td>0</td>\n",
       "      <td>Australia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.413800</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>0.178626</td>\n",
       "      <td>2015-01-28 14:21:11</td>\n",
       "      <td>2015-02-11 20:28:28</td>\n",
       "      <td>-1.197335</td>\n",
       "      <td>PKYOWQKWGJNJI</td>\n",
       "      <td>SEO</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>F</td>\n",
       "      <td>0.101168</td>\n",
       "      <td>16842045</td>\n",
       "      <td>0</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.180852</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>1.649372</td>\n",
       "      <td>2015-03-19 20:49:09</td>\n",
       "      <td>2015-04-11 23:41:23</td>\n",
       "      <td>0.385831</td>\n",
       "      <td>LVCSXLISZHVUO</td>\n",
       "      <td>Ads</td>\n",
       "      <td>IE</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.479197</td>\n",
       "      <td>16843656</td>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.936126</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>-1.131594</td>\n",
       "      <td>2015-02-24 06:11:57</td>\n",
       "      <td>2015-05-23 16:40:14</td>\n",
       "      <td>0.986342</td>\n",
       "      <td>UHAUHNXXUADJE</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>F</td>\n",
       "      <td>-0.363124</td>\n",
       "      <td>16938732</td>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.867086</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>-0.217963</td>\n",
       "      <td>2015-07-07 12:58:11</td>\n",
       "      <td>2015-11-03 04:04:30</td>\n",
       "      <td>0.767974</td>\n",
       "      <td>XPGPMOHIDRMGE</td>\n",
       "      <td>SEO</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>F</td>\n",
       "      <td>0.449387</td>\n",
       "      <td>16971984</td>\n",
       "      <td>0</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.700633</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id         signup_time       purchase_time  purchase_value  \\\n",
       "634  0.411032 2015-06-28 03:00:34 2015-08-09 03:57:29        0.549607   \n",
       "635  0.178626 2015-01-28 14:21:11 2015-02-11 20:28:28       -1.197335   \n",
       "636  1.649372 2015-03-19 20:49:09 2015-04-11 23:41:23        0.385831   \n",
       "637 -1.131594 2015-02-24 06:11:57 2015-05-23 16:40:14        0.986342   \n",
       "638 -0.217963 2015-07-07 12:58:11 2015-11-03 04:04:30        0.767974   \n",
       "\n",
       "         device_id  source browser sex       age  ip_address  class  \\\n",
       "634  KIXYSVCHIPQBR     SEO  Safari   F -0.363124    16778864      0   \n",
       "635  PKYOWQKWGJNJI     SEO  Chrome   F  0.101168    16842045      0   \n",
       "636  LVCSXLISZHVUO     Ads      IE   M -0.479197    16843656      0   \n",
       "637  UHAUHNXXUADJE  Direct  Chrome   F -0.363124    16938732      0   \n",
       "638  XPGPMOHIDRMGE     SEO  Chrome   F  0.449387    16971984      0   \n",
       "\n",
       "       country  transaction_frequency  transaction_velocity  hour_of_day  \\\n",
       "634  Australia                    0.0             -0.413800            3   \n",
       "635   Thailand                    0.0             -1.180852           20   \n",
       "636      China                    0.0             -0.936126           23   \n",
       "637      China                    0.0              0.867086           16   \n",
       "638   Thailand                    0.0              1.700633            4   \n",
       "\n",
       "     day_of_week  \n",
       "634            6  \n",
       "635            2  \n",
       "636            5  \n",
       "637            5  \n",
       "638            1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data_df = encode_categorical_features(fraud_data_df)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify categorical columns\n",
    "# categorical_columns = fraud_data_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# # One-Hot Encode categorical features\n",
    "# fraud_data_df = pd.get_dummies(fraud_data_df, columns=categorical_columns, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = fraud_data_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "# Apply Label Encoding for each categorical column\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    fraud_data_df[col] = label_encoders[col].fit_transform(fraud_data_df[col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 283726 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    283726 non-null  float64\n",
      " 1   V1      283726 non-null  float64\n",
      " 2   V2      283726 non-null  float64\n",
      " 3   V3      283726 non-null  float64\n",
      " 4   V4      283726 non-null  float64\n",
      " 5   V5      283726 non-null  float64\n",
      " 6   V6      283726 non-null  float64\n",
      " 7   V7      283726 non-null  float64\n",
      " 8   V8      283726 non-null  float64\n",
      " 9   V9      283726 non-null  float64\n",
      " 10  V10     283726 non-null  float64\n",
      " 11  V11     283726 non-null  float64\n",
      " 12  V12     283726 non-null  float64\n",
      " 13  V13     283726 non-null  float64\n",
      " 14  V14     283726 non-null  float64\n",
      " 15  V15     283726 non-null  float64\n",
      " 16  V16     283726 non-null  float64\n",
      " 17  V17     283726 non-null  float64\n",
      " 18  V18     283726 non-null  float64\n",
      " 19  V19     283726 non-null  float64\n",
      " 20  V20     283726 non-null  float64\n",
      " 21  V21     283726 non-null  float64\n",
      " 22  V22     283726 non-null  float64\n",
      " 23  V23     283726 non-null  float64\n",
      " 24  V24     283726 non-null  float64\n",
      " 25  V25     283726 non-null  float64\n",
      " 26  V26     283726 non-null  float64\n",
      " 27  V27     283726 non-null  float64\n",
      " 28  V28     283726 non-null  float64\n",
      " 29  Amount  283726 non-null  float64\n",
      " 30  Class   283726 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 69.3 MB\n"
     ]
    }
   ],
   "source": [
    "creditcard_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 129146 entries, 634 to 131728\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   user_id                129146 non-null  float64       \n",
      " 1   signup_time            129146 non-null  datetime64[ns]\n",
      " 2   purchase_time          129146 non-null  datetime64[ns]\n",
      " 3   purchase_value         129146 non-null  float64       \n",
      " 4   device_id              129146 non-null  int64         \n",
      " 5   source                 129146 non-null  int64         \n",
      " 6   browser                129146 non-null  int64         \n",
      " 7   sex                    129146 non-null  int64         \n",
      " 8   age                    129146 non-null  float64       \n",
      " 9   ip_address             129146 non-null  int64         \n",
      " 10  fraud_class            129146 non-null  int64         \n",
      " 11  country                129146 non-null  int64         \n",
      " 12  transaction_frequency  129146 non-null  float64       \n",
      " 13  transaction_velocity   129146 non-null  float64       \n",
      " 14  hour_of_day            129146 non-null  int32         \n",
      " 15  day_of_week            129146 non-null  int32         \n",
      "dtypes: datetime64[ns](2), float64(5), int32(2), int64(7)\n",
      "memory usage: 15.8 MB\n"
     ]
    }
   ],
   "source": [
    "fraud_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Data Results:\n",
      "\n",
      "Model: Logistic Regression\n",
      "Training Time: 18.39 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.65      0.77     23427\n",
      "           1       0.17      0.70      0.27      2403\n",
      "\n",
      "    accuracy                           0.65     25830\n",
      "   macro avg       0.56      0.67      0.52     25830\n",
      "weighted avg       0.88      0.65      0.72     25830\n",
      "\n",
      "ROC AUC: 0.7581122229844558\n",
      "\n",
      "Model: Decision Tree\n",
      "Training Time: 4.24 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95     23427\n",
      "           1       0.48      0.57      0.52      2403\n",
      "\n",
      "    accuracy                           0.90     25830\n",
      "   macro avg       0.72      0.75      0.73     25830\n",
      "weighted avg       0.91      0.90      0.91     25830\n",
      "\n",
      "ROC AUC: 0.7523471189250088\n",
      "\n",
      "Model: Random Forest\n",
      "Training Time: 60.04 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     23427\n",
      "           1       1.00      0.54      0.70      2403\n",
      "\n",
      "    accuracy                           0.96     25830\n",
      "   macro avg       0.98      0.77      0.84     25830\n",
      "weighted avg       0.96      0.96      0.95     25830\n",
      "\n",
      "ROC AUC: 0.7666520010869156\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Training Time: 55.11 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     23427\n",
      "           1       1.00      0.54      0.70      2403\n",
      "\n",
      "    accuracy                           0.96     25830\n",
      "   macro avg       0.98      0.77      0.84     25830\n",
      "weighted avg       0.96      0.96      0.95     25830\n",
      "\n",
      "ROC AUC: 0.7801389076960383\n",
      "\n",
      "Model: MLP\n",
      "Training Time: 49.33 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.48      0.64     23427\n",
      "           1       0.13      0.77      0.23      2403\n",
      "\n",
      "    accuracy                           0.51     25830\n",
      "   macro avg       0.54      0.63      0.43     25830\n",
      "weighted avg       0.88      0.51      0.60     25830\n",
      "\n",
      "ROC AUC: 0.7196708358941699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore convergence warnings for MLP\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Rename the 'class' column to avoid conflict with Python's reserved keywords\n",
    "fraud_data_df.rename(columns={'class': 'fraud_class'}, inplace=True)\n",
    "\n",
    "# Data Preparation function (removing standardization, as you've done it already)\n",
    "def prepare_data(df, target_col, drop_cols):\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Model Training and Evaluation function\n",
    "def evaluate_models(X_train, X_test, y_train, y_test, models):\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            print(f\"Training {name}...\")\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            end_time = time.time()\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            print(f\"Model: {name}\")\n",
    "            print(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "\n",
    "            # ROC AUC score calculation (for models that support predict_proba)\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba)}\\n\")\n",
    "            else:\n",
    "                print(f\"{name} does not support probability prediction.\\n\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error with model {name}: {str(e)}\\n\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=2000, class_weight='balanced'),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50),  # Simplified for faster training\n",
    "    'MLP': MLPClassifier(max_iter=1000),  # Increased iterations\n",
    "}\n",
    "\n",
    "# Fraud data processing\n",
    "X_fraud, y_fraud = prepare_data(fraud_data_df, 'fraud_class', ['fraud_class', 'signup_time', 'purchase_time', 'ip_address'])\n",
    "\n",
    "# Convert target variable to integers (in case they are float)\n",
    "y_fraud = y_fraud.astype(int)\n",
    "\n",
    "# Standardize the features for MLP and models that require scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(X_fraud, y_fraud, test_size=0.2, random_state=42)\n",
    "X_train_fraud_scaled = scaler.fit_transform(X_train_fraud)\n",
    "X_test_fraud_scaled = scaler.transform(X_test_fraud)\n",
    "\n",
    "# Evaluate models on fraud data (use scaled data for MLP)\n",
    "print(\"Fraud Data Results:\\n\")\n",
    "evaluate_models(X_train_fraud_scaled, X_test_fraud_scaled, y_train_fraud, y_test_fraud, models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_fraud.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fraud_data_df['fraud_class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Credit card data processing\n",
    "# Assuming creditcard_data is a DataFrame and 'Class' is the target\n",
    "X_creditcard, y_creditcard = prepare_data(creditcard_data, 'Class', ['Class'])\n",
    "X_train_creditcard, X_test_creditcard, y_train_creditcard, y_test_creditcard = train_test_split(X_creditcard, y_creditcard, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate models on credit card data\n",
    "print(\"Credit Card Data Results:\\n\")\n",
    "evaluate_models(X_train_creditcard, X_test_creditcard, y_train_creditcard, y_test_creditcard, models)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
